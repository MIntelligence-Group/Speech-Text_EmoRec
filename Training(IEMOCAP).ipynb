{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the text features, x_train_seq contains the processed text feature vector and y contains \n",
    "##  the respective emotions\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_seq_trunc,y,test_size=0.25,random_state=4)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Splitting the speech features, M contains the processed speech feature vector and l contains \n",
    "##  the respective emotions\n",
    "M_train, M_test, l_train, l_test = train_test_split(M,l,test_size=0.25,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Flatten,GRU,LSTM,Dense,Activation,Input,Dropout,Embedding,concatenate\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input1   = Input(shape=(180,), dtype='float32')\n",
    "_input12   = Dense(340,activation='relu')(_input1)\n",
    "_input13   = Dropout(0.3)(_input12)\n",
    "_input14   = Dense(470, activation='relu')(_input13)\n",
    "_input15   = Dense(600, activation='relu')(_input14)\n",
    "_input2   =  Input(shape=(MAX_LEN,))\n",
    "_input21   = Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN,weights=[emb_matrix],trainable=False)(_input2)\n",
    "_input22   = LSTM(50,return_sequences=True)(_input21)\n",
    "_input23   = LSTM(50,return_sequences=True)(_input22)\n",
    "_input24   = Flatten()(_input23)\n",
    "_input25   = Dropout(0.3)(_input24)\n",
    "\n",
    "output1    = concatenate([_input15,_input25])\n",
    "output2    = Dropout(0.3)(output1)\n",
    "output3    = Dense(500, activation='relu')(output2)\n",
    "output3    = Dense(4, activation='softmax')(output3)\n",
    "\n",
    "model = keras.Model(inputs=[_input1,_input2], outputs=[output3])\n",
    "model.summary()\n",
    "plot_model(model, to_file='multiple_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit([M_train,x_train], y_train, epochs=150, batch_size=64,validation_data=([M_test,x_test], y_test),verbose=2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy = model.evaluate([M_test,x_test], y_test)\n",
    "print('Accuracy on test data: %.2f' % (accuracy*100))\n",
    "_, accu = model.evaluate([M_train,x_train], y_train)\n",
    "print('Accuracy on train data: %.2f' % (accu*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.title('model Loss')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model.save(\"iemocap.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics         #confusion matrix\n",
    "prediction=t.predict([M_test,x_test])\n",
    "y_pred=(np.argmax(prediction, axis=1))\n",
    "y_test=y_test.reshape((234,))\n",
    "y_test=y_test.astype('int64') \n",
    "c = metrics.confusion_matrix(y_pred, y_test)\n",
    "normed_c = (c.T / c.astype(np.float).sum(axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "#fig= plt.figure(figsize=(11,11))\n",
    "fig, (ax1) = plt.subplots(1,1,figsize=(6,4))\n",
    "sb.set(font_scale=1)\n",
    "x_axis_labels = [\"Angry\",\"Happy\",\"Sad\",\"Neutral\"] # labels for x-axis\n",
    "y_axis_labels = [\"Angry\",\"Happy\",\"Sad\",\"Neutral\"]\n",
    "#akws = {\"ha\": 'left',\"va\": 'top'}\n",
    "cbar_kws = {\"orientation\":\"vertical\",  \n",
    "           }\n",
    "#plt.subplot(1,3,1)\n",
    "sb.heatmap(normed_c,annot=True,cmap=\"YlGnBu\",fmt='.3f',cbar_kws=cbar_kws,xticklabels=x_axis_labels,linewidths=1, yticklabels=y_axis_labels,ax=ax1)\n",
    "#plt.subplot(1,3,2)\n",
    "ax1.tick_params(rotation=0,labelsize=11.5)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.savefig('confmat.pdf',bbox_inches =\"tight\", \n",
    "            pad_inches = 1, \n",
    "            orientation ='landscape',dpi=1200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
